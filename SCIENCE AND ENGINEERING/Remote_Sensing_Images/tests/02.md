# Module 2 Test questions and your answers
![](https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera-course-photos.s3.amazonaws.com/b8/f13d23685c4f8ca8d2a1077826d6b0/Navigation-Thimnail.png?auto=format%2Ccompress&dpr=1&w=256&h=32)

1. This question refers to Figure 1 in the instruction sheet.Suppose you have been asked to design a two-band sensor for discriminating among the cover types shown in the figure. choose all the correct answers below.

- [x] The best band pairs to use are  0.6&nbsp;and 0.9 micrometres

That is correct.  They give the best discrimination among the cover types.
- [ ] The best band pairs to use are  0.9&nbsp;and 1.1 micrometres

- [ ] The best band pairs to use are  0.5&nbsp;and 0.6 micrometres
- [ ] The best three bands to use would be 0.5, 0.6 and 1.1 micrometres
- [ ] A 0.6 micrometre band will always allow good soil-vegetation discrimination because of the chlorophyll content of the vegetation
- [x] A single band at 1.0 micrometre is sufficient for separating water from the other cover types

That is correct and is obvious from the diagram.
- [x] A colour image formed using bands at 0.5, 0.6 and 1.1 micrometres, allocated to the primary display colours in the conventional way will show soils as reddish-yellow.

That is correct.  The red and green displayed signals will be strongest
- [ ] A colour image formed using bands at 0.5, 0.6 and 1.1 micrometres, allocated to the primary display colours in the conventional way will show soils as blue

2. Compare the minimum distance and maximum likelihood classifiers and choose the correct answers from the following.

- [x] The decision surfaces for the maximum likelihood classifier are of higher mathematical order than those for the minimum distance classifier.

That is correct.  They are quadratic compared with linear.
- [ ] More training data per class is needed for the minimum distance classifier
- [ ] Prior probabilities can be used with both classifiers
- [ ] The maximum likelihood classifier is faster in general than the minimum  distance classifier.
- [x] The classification time for the maximum likelihood classifier increases quadratically with the number of features.

That is correct.
- [x] The minimum distance classifier does not make use of class covariance matrices

That is correct.  It inly relies on class mean vectors.
- [ ] The maximum likelihood classifier is trained on spectral classes whereas the minimum distance classifier is trained on information classes
- [x] The classes on which the maximum likelihood classifier don't have to be exactly normally distributed
- [x] Both the minimum distance and maximum likelihood classifiers are multi-class classifiers.

That is correct

3. Consider the properties and structure of the convolutional neural network and choose the correct answers from the following.

- [ ] Back-propagation as a training process can only be used with fully connected neural networks
- [ ] A convolutional neural network with the same structure as a fully connected network has the same number of unknown weights to be found by training.
- [x] Pooling layers in a convolutional neural network help emphasise  the spatial neighbourhood information in an image

That is correct
- [x] Pooling layers in a convolutional neural network reduce the number of unknown weights to be found through training.

That is correct.
- [ ] Deep learning refers to the number of output nodes in a convolutional neural network.
- [x] The neural network succeeds as a classifier by implementing a very large number of linear decisions 

That is correct.  Each processing element computes a linear function of the inputs from the preceding layer.
- [ ] The ReLU function is used because the normal sigmoid is just too hard to compute.
- [ ] It is better to use a convolutional neural network as a classifier than the support vector machine (SVM) because it can discover structure in image data not found by the SVM
- [ ] Deep learning refers to the number of parallel paths (filters) used in a convolutional neural network.
- [ ] The Hughes phenomenon is a consideration for all classifiers.

4. This question relates to unsupervised classification and clustering.  Choose all the correct answers below.

- [ ] Clustering is a good way to find the information classes in image data

- [x] The cluster classes found by Isodata clustering can be used in a maximum likelihood classifier.

That is correct.  In fact the clusters generated by any clustering process can be used that way
- [ ] The benefit of k means clustering compared with other methods is that it produces a unique result.
- [ ] The k means clustering algorithm cannot be used with hyperspectral data sets because it can't cope with large numbers of bands
- [x] The K trees method of clustering is better than the k means method with big data sets because it is much faster

That is correct
- [ ] The cluster classes found by the single pass clustering algorithm cannot be used in a minimum distance classifier.
- [ ] Unsupervised classification is used when the analyst does not have confidence in the supervised algorithms available
- [x] Clustering is a good method to use to discover the spectral or data class structure of a given image data set.

That is correct and is one of the key reasons clustering is used in remote sensing image analysis.
- [ ] The key benefit ox mixing clustering (unsupervised classification) with a supervised algorithm is that a good mapping of spectral and information classes is found, and the process is time efficient.

5. Choose the correct answers below.

- [ ] A convolutional neural network with a stride of 3 at one layer will increase the number of unknown weights to be found 
- [ ] Weight re-use in a convolutional neural network means using the same set of local weights to connect each neighbourhood in one layer to a process element in the next layer.
- [x] Bayes Theorem relates the class conditional and posterior probabilities.

That is correct.
- [ ] The prior probability of class membership is generated by the maximum likelihood classifier but not be the support vector machine.

- [ ] A linear classifier can also generate posterior probabilities.
- [x] The support vector classifier is a fast algorithm because it is based on sets of linear decisions.

That is correct.
- [x] The only purpose of the kernel function in the support vector classifier is to allow it to handle data that is not linearly separable.

That is correct.
- [x] Linear class overlap in the support vector classifier is handled using slack variables.

That is correct.
- [ ] Grid searching is a technique used with the support vector machine to find values for the slack variables. 


- [ ] The use of second order statistics (covariance matrices) with the maximum likelihood classifier leads to problems with parameter estimation on small training sets that is not so severe with linear classifiers such as the support vector machine.
- [ ] The margin in the support vector classifier is the margin of error tolerable by the analyst.


That is incorrect.  The margin is the space between two classes of data.
- [ ] The support vector machine is the best classifier to use in remote sensing.
